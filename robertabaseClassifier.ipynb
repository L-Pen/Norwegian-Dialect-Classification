{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juste\\OneDrive\\Desktop\\University\\COMP 550\\Final Project\\comp550-norwegian-dialects\\treebanks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\juste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\juste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from conllu import parse\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "notebook_path = Path().resolve()\n",
    "code_folder = notebook_path.parent\n",
    "treebanks_folder = code_folder / \"treebanks\"\n",
    "print(treebanks_folder)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "norwegian_stop_words = set(stopwords.words(\"norwegian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conll_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        conll_data = file.read()\n",
    "        return parse(conll_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juste\\OneDrive\\Desktop\\University\\COMP 550\\Final Project\\comp550-norwegian-dialects\\treebanks\n",
      "TokenList<kor, gammal, var, du, aal_uio_0201, da, du, begynte, å, køyre, tømmer, ?, metadata={text: \"kor gammal var du aal_uio_0201 da du begynte å køyre tømmer ?\", segstart: \"42.422\", segstop: \"45.249\", file: \"aal_uio_02\", speaker: \"khs\", id: \"2\"}>\n",
      "# text = kor gammal var du aal_uio_0201 da du begynte å køyre tømmer ?\n",
      "# segstart = 42.422\n",
      "# segstop = 45.249\n",
      "# file = aal_uio_02\n",
      "# speaker = khs\n",
      "# id = 2\n",
      "1\tkor\tkor\tadv\tadv\t_\t2\tADV\t_\t_\n",
      "2\tgammal\tgammal\tadj\tadj\teint|m/f|ub|pos\t3\tSPRED\t_\t_\n",
      "3\tvar\tvere\tverb\tverb\tpret\t0\tFINV\t_\t_\n",
      "4\tdu\tdu\tpron\tpron\tpers|eint|2|nom|hum\t3\tSUBJ\t_\t_\n",
      "5\taal_uio_0201\taal_uio_0201\tsubst\tsubst\tprop\t4\tAPP\t_\t_\n",
      "6\tda\tda\tsbu\tsbu\t_\t8\tSBU\t_\t_\n",
      "7\tdu\tdu\tpron\tpron\tpers|eint|2|nom|hum\t8\tSUBJ\t_\t_\n",
      "8\tbegynte\tbegynne\tverb\tverb\tpret\t3\tADV\t_\t_\n",
      "9\tå\tå\tinf-merke\tinf-merke\t_\t8\tDOBJ\t_\t_\n",
      "10\tkøyre\tkøyre\tverb\tverb\tinf\t9\tINFV\t_\t_\n",
      "11\ttømmer\ttømmer\tsubst\tsubst\tappell|fl|nøyt|ub\t10\tDOBJ\t_\t_\n",
      "12\t?\t$?\tclb\tclb\t<spm>\t3\tIP\t_\t_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In this function we will have a nested list all_parsed_data (nested list of sentences where the outter list is a specific dialect while the inner list is the sentences per dialect)\n",
    "#The second list all_parsed_data_filtered is the filtered version of the same thing\n",
    "#Each element in the inner list is a token and there is meta data at the end\n",
    "#Each token has associated info lemma,upos,etc\n",
    "\n",
    "all_parsed_data = [] \n",
    "print(treebanks_folder)\n",
    "\n",
    "for file_path in treebanks_folder.glob(\"*.conll\"):\n",
    "    parsed_data = parse_conll_file(file_path)\n",
    "    all_parsed_data.append(parsed_data)\n",
    "\n",
    "if all_parsed_data:\n",
    "    first_file_parsed_data = all_parsed_data[0][1]\n",
    "    serialized = first_file_parsed_data.serialize()\n",
    "    print(first_file_parsed_data)\n",
    "    print(serialized)\n",
    "    token = first_file_parsed_data[0]\n",
    "    # print(token['id'])\n",
    "    # print(token['form'])\n",
    "    # print(token['lemma'])\n",
    "    # print(token['upos'])\n",
    "else:\n",
    "    all_parsed_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roberta_dataset(num_classes):\n",
    "    import pandas as pd\n",
    "    better_data ={}\n",
    "\n",
    "    for dialect in all_parsed_data:\n",
    "\n",
    "        dialect_name = dialect[0].metadata.get('file', '')\n",
    "        better_data[dialect_name] = dialect\n",
    "    \n",
    "    ls_aal = better_data['aal_uio_02'] + better_data['aal_uio_06']\n",
    "    ls_austevoll = better_data['austevoll_uib_01'] + better_data['austevoll_uib_04']\n",
    "\n",
    "    better_data['aal_uio_02'] = ls_aal\n",
    "    better_data['austevoll_uib_01'] = ls_austevoll\n",
    "\n",
    "    del better_data['aal_uio_06']\n",
    "    del better_data['austevoll_uib_04']\n",
    "\n",
    "    temp_data = {}\n",
    "    for dialect, token_list in better_data.items():\n",
    "        temp_ls = []\n",
    "        for token in token_list:\n",
    "            sentence = token.metadata.get('text', '')\n",
    "            if len(token) > 3:\n",
    "                temp_ls.append(sentence)\n",
    "        temp_data[dialect] = temp_ls\n",
    "\n",
    "\n",
    "    len_each_dialect = {}\n",
    "    for dialect, token_list in temp_data.items():\n",
    "        len_each_dialect[dialect] = len(token_list)\n",
    "    \n",
    "    sorted_len_each_dialect = sorted(len_each_dialect.items(), key=lambda x: x[1], reverse=True)[:num_classes]\n",
    "\n",
    "    filtered_dict = {key[0]: temp_data[key[0]] for key in sorted_len_each_dialect}\n",
    "    \n",
    "    #Aaustevoll, Bardu, Brandbu, Eidsberg, Fana, Lista, Flakstad, Førde, Giske, Gol, Hemsedal, Herad, Hjartdal, Høyanger, Nordli, Vardø and Ål\n",
    "    map_names = {\n",
    "        'bardu_uit_01': 'Bardu',\n",
    "        'brandbu_uio_01_ny': 'Brandbu',\n",
    "        'eidsberg_uio_03': 'Eidsberg',\n",
    "        'fana_uib_03': 'Fana',\n",
    "        'farsund_uib_02': 'Farsund',\n",
    "        'flakstad_uib_04': 'Flakstad', \n",
    "        'foerde_uib_05': 'Foerde',\n",
    "        'giske_uib_02': 'Giske',\n",
    "        'gol_uio_01': 'Gol',\n",
    "        'hemsedal_uio_01': 'Hemsedal',\n",
    "        'herad_uio_01': 'Herad',\n",
    "        'hjartdal_uio_01': 'Hjartdal',\n",
    "        'hoeyanger_uib_02': 'Hoeyanger',\n",
    "        'lierne_uio_01': 'Lierne', \n",
    "        'vardoe_uio_01': 'Vardoe',\n",
    "        'aal_uio_02': 'Al', \n",
    "        'austevoll_uib_01': 'Austevoll',\n",
    "    }\n",
    "    dialect_to_index = {dialect: idx for idx, dialect in enumerate(filtered_dict.keys())}\n",
    "    list_of_names_add_to_prompt = \", \".join([map_names[key] for key in filtered_dict.keys()])\n",
    "    # print(list_of_names_add_to_prompt)\n",
    "    prompt = f\"You are tasked with being a Norwegian Dialect classifier. The goal is to train a model that can accurately distinguish between different Norwegian dialects. The primary dialects of interest are {list_of_names_add_to_prompt} and you should be able to distinguish between these dialects.\\n Here is the sentence: \"\n",
    "    \n",
    "    test_size = 0.2\n",
    "\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "\n",
    "    for dialect, sentence_list in filtered_dict.items():\n",
    "        label = dialect_to_index[dialect]\n",
    "        train_size = int(len(sentence_list) * (1 - test_size))\n",
    "        train_dataset.extend([{\"label\": label, \"text\": sentence} for sentence in sentence_list[:train_size]])\n",
    "        test_dataset.extend([{\"label\": label, \"text\": sentence} for sentence in sentence_list[train_size:]])\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_roberta_dataset3, test_roberta_dataset3 = create_roberta_dataset(3)\n",
    "train_roberta_dataset6, test_roberta_dataset6 = create_roberta_dataset(6)\n",
    "train_roberta_dataset12, test_roberta_dataset12 = create_roberta_dataset(12)\n",
    "train_roberta_dataset17, test_roberta_dataset17 = create_roberta_dataset(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\juste\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "def tokenzie_data(train_roberta_dataset,test_roberta_dataset):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/norwegian-roberta-base\")\n",
    "\n",
    "    data_training = {\n",
    "        \"label\": [one[\"label\"] for one in train_roberta_dataset],\n",
    "        \"text\": [one[\"text\"] for one in train_roberta_dataset],\n",
    "    }\n",
    "    data_testing = {\n",
    "        \"label\": [one[\"label\"] for one in test_roberta_dataset],\n",
    "        \"text\": [one[\"text\"] for one in test_roberta_dataset],\n",
    "    }\n",
    "\n",
    "    num_classes = len(set(data_training[\"label\"]))\n",
    "    data_training[\"label\"] = [torch.nn.functional.one_hot(torch.tensor(label), num_classes=num_classes) for label in data_training[\"label\"]]\n",
    "    data_testing[\"label\"] = [torch.nn.functional.one_hot(torch.tensor(label), num_classes=num_classes) for label in data_testing[\"label\"]]\n",
    "\n",
    "    dataset_training = Dataset.from_dict(data_training)\n",
    "    dataset_testing = Dataset.from_dict(data_testing)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=50)\n",
    "\n",
    "    tokenized_dataset_training = dataset_training.map(tokenize_function)\n",
    "    tokenized_dataset_testing = dataset_testing.map(tokenize_function)\n",
    "    return tokenized_dataset_training, tokenized_dataset_testing, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_roberta_model(tokenized_dataset_training,tokenized_dataset_testing,num_classes):\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"patrickvonplaten/norwegian-roberta-base\", num_labels=num_classes)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"test_trainer\",\n",
    "        evaluation_strategy=\"epoch\",      \n",
    "    )\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        references = np.argmax(labels, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset_training,\n",
    "        eval_dataset=tokenized_dataset_testing,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1665/1665 [00:00<00:00, 2861.01 examples/s]\n",
      "Map: 100%|██████████| 418/418 [00:00<00:00, 2548.78 examples/s]\n",
      "Map: 100%|██████████| 2595/2595 [00:00<00:00, 3409.98 examples/s]\n",
      "Map: 100%|██████████| 653/653 [00:00<00:00, 2940.95 examples/s]\n",
      "Map: 100%|██████████| 3999/3999 [00:01<00:00, 3570.54 examples/s]\n",
      "Map: 100%|██████████| 1006/1006 [00:00<00:00, 3398.62 examples/s]\n",
      "Map: 100%|██████████| 4748/4748 [00:01<00:00, 3486.05 examples/s]\n",
      "Map: 100%|██████████| 1196/1196 [00:00<00:00, 3340.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset_training3, tokenized_dataset_testing3, num_classes3 = tokenzie_data(train_roberta_dataset3, test_roberta_dataset3)\n",
    "tokenized_dataset_training6, tokenized_dataset_testing6, num_classes6 = tokenzie_data(train_roberta_dataset6, test_roberta_dataset6)\n",
    "tokenized_dataset_training12, tokenized_dataset_testing12, num_classes12 = tokenzie_data(train_roberta_dataset12, test_roberta_dataset12)\n",
    "tokenized_dataset_training17, tokenized_dataset_testing17, num_classes17 = tokenzie_data(train_roberta_dataset17, test_roberta_dataset17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = Path().resolve()\n",
    "code_folder = notebook_path.parent\n",
    "robertamodel_folder = code_folder / \"robertamodels3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at patrickvonplaten/norwegian-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 33%|███▎      | 209/627 [08:10<13:22,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38422995805740356, 'eval_accuracy': 0.7177033492822966, 'eval_runtime': 22.184, 'eval_samples_per_second': 18.842, 'eval_steps_per_second': 2.389, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 418/627 [16:08<06:41,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3108651340007782, 'eval_accuracy': 0.8301435406698564, 'eval_runtime': 22.1133, 'eval_samples_per_second': 18.903, 'eval_steps_per_second': 2.397, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 500/627 [19:13<04:45,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.39, 'learning_rate': 1.0127591706539077e-05, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 627/627 [23:55<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3572394847869873, 'eval_accuracy': 0.8181818181818182, 'eval_runtime': 19.606, 'eval_samples_per_second': 21.32, 'eval_steps_per_second': 2.703, 'epoch': 3.0}\n",
      "{'train_runtime': 1435.3481, 'train_samples_per_second': 3.48, 'train_steps_per_second': 0.437, 'train_loss': 0.34199358439711675, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "roberta_model3 = run_roberta_model(tokenized_dataset_training3, tokenized_dataset_testing3, num_classes3)\n",
    "roberta_model3.save_pretrained(robertamodel_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at patrickvonplaten/norwegian-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|███▎      | 325/975 [12:11<21:58,  2.03s/it]\n",
      " 33%|███▎      | 325/975 [12:46<21:58,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3586987555027008, 'eval_accuracy': 0.5022970903522205, 'eval_runtime': 34.1217, 'eval_samples_per_second': 19.137, 'eval_steps_per_second': 2.403, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 500/975 [18:37<16:38,  2.10s/it]  Checkpoint destination directory test_trainer\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3591, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 650/975 [23:50<10:57,  2.02s/it]\n",
      " 67%|██████▋   | 650/975 [24:26<10:57,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3284062445163727, 'eval_accuracy': 0.552833078101072, 'eval_runtime': 35.3552, 'eval_samples_per_second': 18.47, 'eval_steps_per_second': 2.319, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975/975 [36:00<00:00,  1.72s/it]  \n",
      "100%|██████████| 975/975 [36:32<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33547210693359375, 'eval_accuracy': 0.5666156202143952, 'eval_runtime': 31.4267, 'eval_samples_per_second': 20.779, 'eval_steps_per_second': 2.609, 'epoch': 3.0}\n",
      "{'train_runtime': 2192.2583, 'train_samples_per_second': 3.551, 'train_steps_per_second': 0.445, 'train_loss': 0.2885448044996995, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "roberta_model6 = run_roberta_model(tokenized_dataset_training6, tokenized_dataset_testing6, num_classes6)\n",
    "robertamodel_folder = code_folder / \"robertamodels6\"\n",
    "roberta_model6.save_pretrained(robertamodel_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at patrickvonplaten/norwegian-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|███▎      | 500/1500 [21:26<47:26,  2.85s/it]  Checkpoint destination directory test_trainer\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2761, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 500/1500 [22:35<47:26,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2622585594654083, 'eval_accuracy': 0.2286282306163022, 'eval_runtime': 67.1149, 'eval_samples_per_second': 14.989, 'eval_steps_per_second': 1.877, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1000/1500 [40:56<18:16,  2.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2281, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 1000/1500 [41:53<18:16,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2428758144378662, 'eval_accuracy': 0.28429423459244535, 'eval_runtime': 55.7947, 'eval_samples_per_second': 18.03, 'eval_steps_per_second': 2.258, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [1:00:24<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.19, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1500/1500 [1:01:12<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23466500639915466, 'eval_accuracy': 0.3628230616302187, 'eval_runtime': 46.7623, 'eval_samples_per_second': 21.513, 'eval_steps_per_second': 2.694, 'epoch': 3.0}\n",
      "{'train_runtime': 3672.8404, 'train_samples_per_second': 3.266, 'train_steps_per_second': 0.408, 'train_loss': 0.23140023803710938, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "roberta_model12 = run_roberta_model(tokenized_dataset_training12, tokenized_dataset_testing12, num_classes12)\n",
    "robertamodel_folder = code_folder / \"robertamodels12\"\n",
    "roberta_model12.save_pretrained(robertamodel_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at patrickvonplaten/norwegian-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 28%|██▊       | 500/1782 [20:49<51:46,  2.42s/it]  Checkpoint destination directory test_trainer\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2261, 'learning_rate': 3.5970819304152635e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 33%|███▎      | 594/1782 [26:09<46:24,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21544848382472992, 'eval_accuracy': 0.1714046822742475, 'eval_runtime': 72.5344, 'eval_samples_per_second': 16.489, 'eval_steps_per_second': 2.068, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1000/1782 [43:30<32:40,  2.51s/it] Checkpoint destination directory test_trainer\\checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2076, 'learning_rate': 2.1941638608305277e-05, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 1188/1782 [52:42<23:34,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19714465737342834, 'eval_accuracy': 0.24665551839464883, 'eval_runtime': 70.0712, 'eval_samples_per_second': 17.068, 'eval_steps_per_second': 2.141, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1500/1782 [1:05:49<11:31,  2.45s/it]Checkpoint destination directory test_trainer\\checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1846, 'learning_rate': 7.912457912457913e-06, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 1782/1782 [1:18:26<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19099938869476318, 'eval_accuracy': 0.29347826086956524, 'eval_runtime': 67.5395, 'eval_samples_per_second': 17.708, 'eval_steps_per_second': 2.221, 'epoch': 3.0}\n",
      "{'train_runtime': 4706.2931, 'train_samples_per_second': 3.027, 'train_steps_per_second': 0.379, 'train_loss': 0.2000620121640121, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "roberta_model17 = run_roberta_model(tokenized_dataset_training17, tokenized_dataset_testing17, num_classes17)\n",
    "robertamodel_folder = code_folder / \"robertamodels17\"\n",
    "roberta_model17.save_pretrained(robertamodel_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
