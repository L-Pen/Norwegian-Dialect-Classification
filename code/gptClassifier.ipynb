{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/liamo/Desktop/School/550/FinalProj/comp550-norwegian-dialects/treebanks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/liamo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/liamo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from conllu import parse\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "notebook_path = Path().resolve()\n",
    "code_folder = notebook_path.parent\n",
    "treebanks_folder = code_folder / \"treebanks\"\n",
    "print(treebanks_folder)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "norwegian_stop_words = set(stopwords.words(\"norwegian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conll_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        conll_data = file.read()\n",
    "        return parse(conll_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/liamo/Desktop/School/550/FinalProj/comp550-norwegian-dialects/treebanks\n",
      "TokenList<frå, #, Førde, ?, metadata={text: \"frå # Førde ?\", segstart: \"10.891\", segstop: \"11.983\", file: \"foerde_uib_05\", speaker: \"int1\", id: \"2\"}>\n",
      "# text = frå # Førde ?\n",
      "# segstart = 10.891\n",
      "# segstop = 11.983\n",
      "# file = foerde_uib_05\n",
      "# speaker = int1\n",
      "# id = 2\n",
      "1\tfrå\tfrå\tprep\tprep\t_\t0\tFRAG\t_\t_\n",
      "2\t#\t#\tpause\tpause\t_\t3\tIK\t_\t_\n",
      "3\tFørde\tFørde\tsubst\tsubst\tprop\t1\tPUTFYLL\t_\t_\n",
      "4\t?\t$?\tclb\tclb\t<spm>\t1\tIP\t_\t_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In this function we will have a nested list all_parsed_data (nested list of sentences where the outter list is a specific dialect while the inner list is the sentences per dialect)\n",
    "#The second list all_parsed_data_filtered is the filtered version of the same thing\n",
    "#Each element in the inner list is a token and there is meta data at the end\n",
    "#Each token has associated info lemma,upos,etc\n",
    "all_parsed_data = [] \n",
    "print(treebanks_folder)\n",
    "\n",
    "for file_path in treebanks_folder.glob(\"*.conll\"):\n",
    "    parsed_data = parse_conll_file(file_path)\n",
    "    all_parsed_data.append(parsed_data)\n",
    "\n",
    "\n",
    "if all_parsed_data:\n",
    "    first_file_parsed_data = all_parsed_data[0][1]\n",
    "    serialized = first_file_parsed_data.serialize()\n",
    "    print(first_file_parsed_data)\n",
    "    print(serialized)\n",
    "    token = first_file_parsed_data[0]\n",
    "    # print(token['id'])\n",
    "    # print(token['form'])\n",
    "    # print(token['lemma'])\n",
    "    # print(token['upos'])\n",
    "else:\n",
    "    all_parsed_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chatgpt_dataset(num_classes):\n",
    "\n",
    "    better_data ={}\n",
    "\n",
    "    for dialect in all_parsed_data:\n",
    "\n",
    "        dialect_name = dialect[0].metadata.get('file', '')\n",
    "        better_data[dialect_name] = dialect\n",
    "    \n",
    "    ls_aal = better_data['aal_uio_02'] + better_data['aal_uio_06']\n",
    "    ls_austevoll = better_data['austevoll_uib_01'] + better_data['austevoll_uib_04']\n",
    "\n",
    "    better_data['aal_uio_02'] = ls_aal\n",
    "    better_data['austevoll_uib_01'] = ls_austevoll\n",
    "\n",
    "    del better_data['aal_uio_06']\n",
    "    del better_data['austevoll_uib_04']\n",
    "\n",
    "    temp_data = {}\n",
    "    for dialect, token_list in better_data.items():\n",
    "        temp_ls = []\n",
    "        for token in token_list:\n",
    "            sentence = token.metadata.get('text', '')\n",
    "            if len(token) > 3:\n",
    "                temp_ls.append(sentence)\n",
    "        temp_data[dialect] = temp_ls\n",
    "\n",
    "\n",
    "    len_each_dialect = {}\n",
    "    for dialect, token_list in temp_data.items():\n",
    "        len_each_dialect[dialect] = len(token_list)\n",
    "    \n",
    "    sorted_len_each_dialect = sorted(len_each_dialect.items(), key=lambda x: x[1], reverse=True)[:num_classes]\n",
    "\n",
    "    filtered_dict = {key[0]: temp_data[key[0]] for key in sorted_len_each_dialect}\n",
    "    \n",
    "    map_names = {\n",
    "        'bardu_uit_01': 'Bardu',\n",
    "        'brandbu_uio_01_ny': 'Brandbu',\n",
    "        'eidsberg_uio_03': 'Eidsberg',\n",
    "        'fana_uib_03': 'Fana',\n",
    "        'farsund_uib_02': 'Farsund',\n",
    "        'flakstad_uib_04': 'Flakstad', \n",
    "        'foerde_uib_05': 'Foerde',\n",
    "        'giske_uib_02': 'Giske',\n",
    "        'gol_uio_01': 'Gol',\n",
    "        'hemsedal_uio_01': 'Hemsedal',\n",
    "        'herad_uio_01': 'Herad',\n",
    "        'hjartdal_uio_01': 'Hjartdal',\n",
    "        'hoeyanger_uib_02': 'Hoeyanger',\n",
    "        'lierne_uio_01': 'Lierne', \n",
    "        'vardoe_uio_01': 'Vardoe',\n",
    "        'aal_uio_02': 'Al', \n",
    "        'austevoll_uib_01': 'Austevoll',\n",
    "    }\n",
    "\n",
    "    list_of_names_add_to_prompt = \", \".join([map_names[key] for key in filtered_dict.keys()])\n",
    "    # print(list_of_names_add_to_prompt)\n",
    "    prompt = f\"\\n Here is the sentence: \"\n",
    "    \n",
    "    test_size = 0.2\n",
    "\n",
    "    train_dataset = []\n",
    "    validation_dataset = []\n",
    "    test_dataset = []\n",
    "\n",
    "    for dialect, sentence_list in filtered_dict.items():\n",
    "        \n",
    "        copy_sentence_list = sentence_list.copy()\n",
    "        random.shuffle(copy_sentence_list)\n",
    "\n",
    "        train_size = int(len(copy_sentence_list) * (1 - test_size))\n",
    "        \n",
    "\n",
    "        train_dataset.extend([{\n",
    "            \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": f'''\n",
    "                You are tasked with being a Norwegian Dialect classifier. \n",
    "                The goal is to train a model that can accurately distinguish between different Norwegian dialects. \n",
    "                The primary dialects of interest are {list_of_names_add_to_prompt} and you should be able to distinguish between these dialects.'''\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"What dialect does this sentence belong to: {sentence}?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"{dialect}\"\n",
    "            }\n",
    "            ]\n",
    "        } for sentence in copy_sentence_list[:train_size*0.8]])\n",
    "\n",
    "        validation_dataset.extend([{\"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": f'You are tasked with being a Norwegian Dialect classifier. The goal is to train a model that can accurately distinguish between different Norwegian dialects. The primary dialects of interest are {list_of_names_add_to_prompt} and you should be able to distinguish between these dialects.'\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"What dialect does this sentence belong to: {sentence}?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"{dialect}\"\n",
    "            }\n",
    "            ]\n",
    "        } for sentence in copy_sentence_list[train_size*0.8:train_size]])\n",
    "\n",
    "        test_dataset.extend([{\"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": f'You are tasked with being a Norwegian Dialect classifier. The goal is to train a model that can accurately distinguish between different Norwegian dialects. The primary dialects of interest are {list_of_names_add_to_prompt} and you should be able to distinguish between these dialects.'\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"What dialect does this sentence belong to: {sentence}?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"{dialect}\"\n",
    "            }\n",
    "            ]\n",
    "        } for sentence in copy_sentence_list[train_size:]])\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "def setup_finetune_files(train_dataset, validation_dataset, test_dataset, num_classes):\n",
    "    folder_name = \"gptdataset\"\n",
    "    with open(os.path.join(folder_name, f\"training_data_{num_classes}.jsonl\"), \"w\") as output_file:\n",
    "        for entry in train_dataset:\n",
    "            json.dump(entry, output_file)\n",
    "            output_file.write(\"\\n\")\n",
    "\n",
    "    with open(os.path.join(folder_name, f\"validation_data_{num_classes}.jsonl\"), \"w\") as output_file:\n",
    "        for entry in validation_dataset:\n",
    "            json.dump(entry, output_file)\n",
    "            output_file.write(\"\\n\")\n",
    "    \n",
    "    with open(os.path.join(folder_name, f\"test_data_{num_classes}.jsonl\"), \"w\") as output_file:\n",
    "        for entry in test_dataset:\n",
    "            json.dump(entry, output_file)\n",
    "            output_file.write(\"\\n\")\n",
    "    return os.path.join(folder_name, f\"training_data_{num_classes}.jsonl\"), os.path.join(folder_name, f\"validation_data_{num_classes}.jsonl\"), os.path.join(folder_name, f\"test_data_{num_classes}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(train_file, validation_file):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"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\" \n",
    "\n",
    "    client = openai.Client(\n",
    "    )\n",
    "    \n",
    "    train_file = openai.files.create(\n",
    "        file=open(train_file, 'rb'),\n",
    "        purpose='fine-tune',\n",
    "    )\n",
    "\n",
    "    train_file_id = train_file.id\n",
    "\n",
    "    validation_file = openai.files.create(\n",
    "        file=open(validation_file, 'rb'),\n",
    "        purpose='fine-tune',\n",
    "    )\n",
    "\n",
    "    validation_file_id = validation_file.id\n",
    "\n",
    "    client.fine_tuning.jobs.create(\n",
    "        training_file=train_file_id,\n",
    "        validation_file=validation_file_id,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chatgpt_dataset_3, validation_dataset_3, test_chatgpt_dataset_3 = create_chatgpt_dataset(3)\n",
    "train_path_3, validation_path_3, test_path_3 = setup_finetune_files(train_chatgpt_dataset_3, validation_dataset_3, test_chatgpt_dataset_3, 3)\n",
    "fine_tune_model(train_path_3, validation_path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chatgpt_dataset_6, validation_dataset_6, test_chatgpt_dataset_6 = create_chatgpt_dataset(6)\n",
    "train_path_6, validation_path_6, test_path_6 = setup_finetune_files(train_chatgpt_dataset_6, validation_dataset_6, test_chatgpt_dataset_6, 6)\n",
    "fine_tune_model(train_path_6, validation_path_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chatgpt_dataset_12, validation_dataset_12, test_chatgpt_dataset_12 = create_chatgpt_dataset(12)\n",
    "train_path_12, validation_path_12, test_path_12 = setup_finetune_files(train_chatgpt_dataset_12, validation_dataset_12, test_chatgpt_dataset_12, 12)\n",
    "fine_tune_model(train_path_12, validation_path_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chatgpt_dataset_17, validation_dataset_17, test_chatgpt_dataset_17 = create_chatgpt_dataset(17)\n",
    "train_path_17, validation_path_17, test_path_17 = setup_finetune_files(train_chatgpt_dataset_17, validation_dataset_17, test_chatgpt_dataset_17, 17)\n",
    "fine_tune_model(train_path_17, validation_path_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-PJ89dRYXWyawbvtSaxAkus0S', created_at=1702580089, error=None, fine_tuned_model='ft:gpt-3.5-turbo-0613:personal::8VpoRtxb', finished_at=1702598026, hyperparameters=Hyperparameters(n_epochs=3, batch_size=6, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-iUdpRZhOIj1jObRe6m5Gu8vD', result_files=['file-DaTGaKDYfzLgyxbjD2KlHPEY'], status='succeeded', trained_tokens=1384029, training_file='file-rnaOF7fZgSeSGEuHlonzbRHJ', validation_file='file-o1zk0OLI32DcsDXNzAgLj6Mp')\n",
      "FineTuningJob(id='ftjob-TIVM0rLZime0lnbggtzr8SSf', created_at=1702580073, error=None, fine_tuned_model='ft:gpt-3.5-turbo-0613:personal::8VpoeBUx', finished_at=1702598039, hyperparameters=Hyperparameters(n_epochs=3, batch_size=4, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-iUdpRZhOIj1jObRe6m5Gu8vD', result_files=['file-6DJdRFrtExRtKA9mfD8CI94s'], status='succeeded', trained_tokens=790773, training_file='file-gRVs0HriqaCzcruapygdsqdD', validation_file='file-WSvORsdg0dttADPv6pOwlfI0')\n",
      "FineTuningJob(id='ftjob-9i8sABIlshftPvSy9p0sTURQ', created_at=1702580056, error=None, fine_tuned_model='ft:gpt-3.5-turbo-0613:personal::8VpeBPxL', finished_at=1702597390, hyperparameters=Hyperparameters(n_epochs=3, batch_size=3, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-iUdpRZhOIj1jObRe6m5Gu8vD', result_files=['file-9xpwXHPOQSdNFz9Pb8mgBlUg'], status='succeeded', trained_tokens=472659, training_file='file-6s4nlNO5UK90G2KH0twb4J8S', validation_file='file-egw0y9tlA98DEXLA2yCShZQc')\n",
      "FineTuningJob(id='ftjob-q0AXMaf7gYvLkGJSIvFIGNka', created_at=1700585626, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=1, batch_size=128, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-iUdpRZhOIj1jObRe6m5Gu8vD', result_files=[], status='cancelled', trained_tokens=None, training_file='file-4MTejeM7rYtKkQtAy7npKd5M', validation_file='file-4fCd0cX1jvGbsENWkkwST16K')\n",
      "FineTuningJob(id='ftjob-lU1hGq9A4tyggrOlQ9mK5kCZ', created_at=1700585245, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=1, batch_size=19, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-iUdpRZhOIj1jObRe6m5Gu8vD', result_files=[], status='cancelled', trained_tokens=None, training_file='file-OdLxkeL1Q6wWxx69SjIyPLMi', validation_file='file-V0E9CmfGlFP83c2NqUAs8hdJ')\n",
      "FineTuningJob(id='ftjob-ppS5QGrW8TKJlQmqblSSA22C', created_at=1700585205, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=1, batch_size=19, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-iUdpRZhOIj1jObRe6m5Gu8vD', result_files=[], status='cancelled', trained_tokens=None, training_file='file-OdLxkeL1Q6wWxx69SjIyPLMi', validation_file='file-V0E9CmfGlFP83c2NqUAs8hdJ')\n"
     ]
    }
   ],
   "source": [
    "client = openai.Client()\n",
    "\n",
    "all_jobs = []\n",
    "for job in client.fine_tuning.jobs.list(\n",
    "    limit=20,\n",
    "):\n",
    "    # Do something with job here\n",
    "    all_jobs.append(job)\n",
    "    print(job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_class_gpt(model_id, input_message):\n",
    "  from openai import OpenAI\n",
    "  client = OpenAI()\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=input_message,\n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_gpt_model(dialect_mapping, num_classes, file_path, model_id):\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    i = 0\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse each line into a dictionary\n",
    "            data = json.loads(line)\n",
    "            label = data[\"messages\"][-1][\"content\"]\n",
    "            # Test 3 classes:\n",
    "            response = test_class_gpt(model_id, data[\"messages\"][:-1])\n",
    "            predicted_label = dict(dict(dict(response)[\"choices\"][0])[\"message\"])[\"content\"]\n",
    "\n",
    "            true_labels.append(label)\n",
    "            predicted_labels.append(predicted_label)\n",
    "            i += 1\n",
    "    return true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metrics(num_classes, dialect_mapping, true_labels, predicted_labels):\n",
    "    accuracy_count = 0\n",
    "    tp_count = [0] * num_classes\n",
    "    fp_count = [0] * num_classes\n",
    "    fn_count = [0] * num_classes\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        try:\n",
    "            if true == pred:\n",
    "                accuracy_count += 1\n",
    "                tp_count[dialect_mapping[true]] += 1\n",
    "            else:\n",
    "                fp_count[dialect_mapping[pred]] += 1\n",
    "                fn_count[dialect_mapping[true]] += 1\n",
    "        except Exception:\n",
    "            try:\n",
    "                if pred.split(\"_\")[0].lower() in true:\n",
    "                    accuracy_count += 1\n",
    "                    tp_count[dialect_mapping[true]] += 1\n",
    "                else:\n",
    "                    for key in dialect_mapping.keys():\n",
    "                        if key.split(\"_\")[0] in pred:\n",
    "                            pred = key\n",
    "                            break\n",
    "                    fp_count[dialect_mapping[pred]] += 1\n",
    "                    fn_count[dialect_mapping[true]] += 1\n",
    "            except Exception:\n",
    "                print(f\"True: {true}, Predicted: {pred}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "    accuracy = accuracy_count / len(true_labels)\n",
    "    precision = [tp / (tp + fp) if (tp + fp) != 0 else 0 for tp, fp in zip(tp_count, fp_count)]\n",
    "    recall = [tp / (tp + fn) if (tp + fn) != 0 else 0 for tp, fn in zip(tp_count, fn_count)]\n",
    "    f1 = [2 * (p * r) / (p + r) if (p + r) != 0 else 0 for p, r in zip(precision, recall)]\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    for i in range(len(tp_count)):\n",
    "        print(f\"Class {i + 1} - Precision: {precision[i]}, Recall: {recall[i]}, F1 Score: {f1[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8397129186602871\n",
      "Class 1 - Precision: 0.8265895953757225, Recall: 0.8411764705882353, F1 Score: 0.8338192419825073\n",
      "Class 2 - Precision: 0.87248322147651, Recall: 0.8552631578947368, F1 Score: 0.8637873754152824\n",
      "Class 3 - Precision: 0.8125, Recall: 0.8125, F1 Score: 0.8125\n"
     ]
    }
   ],
   "source": [
    "dialect_mapping_3 ={\n",
    "    'flakstad_uib_04': 0,\n",
    "    'giske_uib_02': 1,\n",
    "    'eidsberg_uio_03': 2,\n",
    "}\n",
    "\n",
    "test_path_3 = \"gptdataset/test_data_3.jsonl\"\n",
    "true_labels_3, predicted_labels_3 = run_test_gpt_model(dialect_mapping_3, 3, test_path_3, \"ft:gpt-3.5-turbo-0613:personal::8W6ysErn\")\n",
    "run_metrics(3, dialect_mapping_3, true_labels_3, predicted_labels_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_mapping_6 = {\n",
    "        'flakstad_uib_04': 0,\n",
    "        'giske_uib_02': 1,\n",
    "        'eidsberg_uio_03': 2,\n",
    "        'austevoll_uib_01': 3,\n",
    "        'bardu_uit_01': 4,\n",
    "        'vardoe_uio_01': 5\n",
    "    }\n",
    "\n",
    "test_path_6 = \"gptdataset/test_data_6.jsonl\"\n",
    "true_labels_6, predicted_labels_6 = run_test_gpt_model(dialect_mapping_6, 6, test_path_6, \"ft:gpt-3.5-turbo-0613:personal::8VpoeBUx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.666156202143951\n",
      "Class 1 - Precision: 0.7485380116959064, Recall: 0.7529411764705882, F1 Score: 0.750733137829912\n",
      "Class 2 - Precision: 0.6758241758241759, Recall: 0.8092105263157895, F1 Score: 0.7365269461077845\n",
      "Class 3 - Precision: 0.6476190476190476, Recall: 0.7083333333333334, F1 Score: 0.6766169154228856\n",
      "Class 4 - Precision: 0.5684210526315789, Recall: 0.6136363636363636, F1 Score: 0.5901639344262295\n",
      "Class 5 - Precision: 0.5636363636363636, Recall: 0.41333333333333333, F1 Score: 0.47692307692307695\n",
      "Class 6 - Precision: 0.6888888888888889, Recall: 0.4305555555555556, F1 Score: 0.5299145299145299\n"
     ]
    }
   ],
   "source": [
    "run_metrics(6, dialect_mapping_6, true_labels_6, predicted_labels_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: giske_uib_02, Predicted: fana_uib_03\n",
      "True: eidsberg_uio_03, Predicted: fana_uib_03\n",
      "True: eidsberg_uio_03, Predicted: fana_uib_03\n",
      "True: austevoll_uib_01, Predicted: fana_uib_03\n",
      "True: austevoll_uib_01, Predicted: fana_uib_03\n",
      "True: austevoll_uib_01, Predicted: fana_uib_03\n",
      "True: austevoll_uib_01, Predicted: fana_uib_03\n",
      "True: austevoll_uib_01, Predicted: fana_uib_03\n",
      "True: austevoll_uib_01, Predicted: fana_uib_03\n",
      "True: austevoll_uib_01, Predicted: fana_uib_03\n",
      "True: bardu_uit_01, Predicted: fana_uib_03\n",
      "True: bardu_uit_01, Predicted: fana_uib_03\n",
      "True: brandbu_uio_01_ny, Predicted: fana_uib_03\n",
      "True: hjartdal_uio_01, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: farsund_uib_02, Predicted: fana_uib_03\n",
      "True: foerde_uib_05, Predicted: fana_uib_03\n",
      "True: foerde_uib_05, Predicted: fana_uib_03\n",
      "True: foerde_uib_05, Predicted: fana_uib_03\n",
      "True: foerde_uib_05, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: hjartdal_uio_01\n",
      "True: fana_uib_03, Predicted: brandbu_uio_01_ny\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: farsund_uib_02\n",
      "True: fana_uib_03, Predicted: foerde_uib_05\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: foerde_uib_05\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: flakstad_uib_04\n",
      "True: fana_uib_03, Predicted: eidsberg_uio_03\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: giske_uib_02\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: foerde_uib_05\n",
      "True: fana_uib_03, Predicted: foerde_uib_05\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: farsund_uib_02\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: brandbu_uio_01_ny\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: farsund_uib_02\n",
      "True: fana_uib_03, Predicted: giske_uib_02\n",
      "True: fana_uib_03, Predicted: brandbu_uio_01_ny\n",
      "True: fana_uib_03, Predicted: foerde_uib_05\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: bardu_uit_01\n",
      "True: fana_uib_03, Predicted: giske_uib_02\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: farsund_uib_02\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: bardu_uit_01\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: farsund_uib_02\n",
      "True: fana_uib_03, Predicted: vardoe_uio_01\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: fana_uib_03\n",
      "True: fana_uib_03, Predicted: austevoll_uib_01\n",
      "True: fana_uib_03, Predicted: farsund_uib_02\n",
      "True: fana_uib_03, Predicted: farsund_uib_02\n",
      "True: fana_uib_03, Predicted: eidsberg_uio_03\n",
      "Accuracy: 0.4761431411530815\n",
      "Class 1 - Precision: 0.6330935251798561, Recall: 0.5176470588235295, F1 Score: 0.56957928802589\n",
      "Class 2 - Precision: 0.5652173913043478, Recall: 0.6887417218543046, F1 Score: 0.6208955223880598\n",
      "Class 3 - Precision: 0.55, Recall: 0.46808510638297873, F1 Score: 0.5057471264367817\n",
      "Class 4 - Precision: 0.25, Recall: 0.4444444444444444, F1 Score: 0.32\n",
      "Class 5 - Precision: 0.4878048780487805, Recall: 0.273972602739726, F1 Score: 0.3508771929824562\n",
      "Class 6 - Precision: 0.5, Recall: 0.3333333333333333, F1 Score: 0.4\n",
      "Class 7 - Precision: 0.4788732394366197, Recall: 0.4927536231884058, F1 Score: 0.4857142857142857\n",
      "Class 8 - Precision: 0.35, Recall: 0.3442622950819672, F1 Score: 0.3471074380165289\n",
      "Class 9 - Precision: 0.44871794871794873, Recall: 0.5833333333333334, F1 Score: 0.5072463768115941\n",
      "Class 10 - Precision: 0.2328767123287671, Recall: 0.3541666666666667, F1 Score: 0.2809917355371901\n",
      "Class 11 - Precision: 0.3291139240506329, Recall: 0.5416666666666666, F1 Score: 0.4094488188976378\n",
      "Class 12 - Precision: 0, Recall: 0, F1 Score: 0\n"
     ]
    }
   ],
   "source": [
    "dialect_mapping_12 = {\n",
    "    'flakstad_uib_04': 0,\n",
    "    'giske_uib_02': 1,\n",
    "    'eidsberg_uio_03': 2,\n",
    "    'austevoll_uib_01': 3,\n",
    "    'bardu_uit_01': 4,\n",
    "    'vardoe_uio_01': 5,\n",
    "    'aal_uio_02': 6,\n",
    "    'brandbu_uio_01_ny': 7,\n",
    "    'hjartdal_uio_01': 8,\n",
    "    'farsund_uib_02': 9,\n",
    "    'foerde_uib_05': 10,\n",
    "    'lierne_uio_01': 11\n",
    "}\n",
    "\n",
    "test_path_12 = \"gptdataset/test_data_12.jsonl\"\n",
    "true_labels_12, predicted_labels_12 = run_test_gpt_model(dialect_mapping_12, 12, test_path_12, \"ft:gpt-3.5-turbo-0613:personal::8VpoRtxb\")\n",
    "run_metrics(12, dialect_mapping_12, true_labels_12, predicted_labels_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: eidsberg_uio_03, Predicted: hoyanger_uib_02\n",
      "True: brandbu_uio_01_ny, Predicted: hogne\n",
      "True: fana_uib_03, Predicted: hoyanger_uib_09\n",
      "Accuracy: 0.3762541806020067\n",
      "Class 1 - Precision: 0.5777777777777777, Recall: 0.4588235294117647, F1 Score: 0.5114754098360657\n",
      "Class 2 - Precision: 0.5346534653465347, Recall: 0.7105263157894737, F1 Score: 0.6101694915254237\n",
      "Class 3 - Precision: 0.40350877192982454, Recall: 0.4842105263157895, F1 Score: 0.44019138755980863\n",
      "Class 4 - Precision: 0.19318181818181818, Recall: 0.19318181818181818, F1 Score: 0.19318181818181818\n",
      "Class 5 - Precision: 0.4642857142857143, Recall: 0.17333333333333334, F1 Score: 0.2524271844660194\n",
      "Class 6 - Precision: 0.375, Recall: 0.375, F1 Score: 0.375\n",
      "Class 7 - Precision: 0.417910447761194, Recall: 0.4057971014492754, F1 Score: 0.411764705882353\n",
      "Class 8 - Precision: 0.20454545454545456, Recall: 0.14754098360655737, F1 Score: 0.17142857142857143\n",
      "Class 9 - Precision: 0.2980769230769231, Recall: 0.5081967213114754, F1 Score: 0.3757575757575758\n",
      "Class 10 - Precision: 0.16666666666666666, Recall: 0.13559322033898305, F1 Score: 0.14953271028037385\n",
      "Class 11 - Precision: 0.325, Recall: 0.5, F1 Score: 0.393939393939394\n",
      "Class 12 - Precision: 0.25806451612903225, Recall: 0.16, F1 Score: 0.19753086419753085\n",
      "Class 13 - Precision: 0.35294117647058826, Recall: 0.3673469387755102, F1 Score: 0.36000000000000004\n",
      "Class 14 - Precision: 0.3090909090909091, Recall: 0.3469387755102041, F1 Score: 0.3269230769230769\n",
      "Class 15 - Precision: 0.3103448275862069, Recall: 0.28125, F1 Score: 0.2950819672131148\n",
      "Class 16 - Precision: 0.13636363636363635, Recall: 0.0967741935483871, F1 Score: 0.11320754716981131\n",
      "Class 17 - Precision: 0.17391304347826086, Recall: 0.14285714285714285, F1 Score: 0.1568627450980392\n"
     ]
    }
   ],
   "source": [
    "dialect_mapping_17 = {\n",
    "    'flakstad_uib_04': 0,\n",
    "    'giske_uib_02': 1,\n",
    "    'eidsberg_uio_03': 2,\n",
    "    'austevoll_uib_01': 3,\n",
    "    'bardu_uit_01': 4,\n",
    "    'vardoe_uio_01': 5,\n",
    "    'aal_uio_02': 6,\n",
    "    'brandbu_uio_01_ny': 7,\n",
    "    'hjartdal_uio_01': 8,\n",
    "    'farsund_uib_02': 9,\n",
    "    'foerde_uib_05': 10,\n",
    "    'lierne_uio_01': 11,\n",
    "    'fana_uib_03': 12,\n",
    "    'hemsedal_uio_01': 13,\n",
    "    'herad_uio_01': 14,\n",
    "    'gol_uio_01': 15,\n",
    "    'hoeyanger_uib_02': 16\n",
    "}\n",
    "\n",
    "test_path_17 = \"gptdataset/test_data_17.jsonl\"\n",
    "true_labels_17, predicted_labels_17 = run_test_gpt_model(dialect_mapping_17, 17, test_path_17, \"ft:gpt-3.5-turbo-0613:personal::8W7Bc36I\")\n",
    "run_metrics(17, dialect_mapping_17, true_labels_17, predicted_labels_17)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
