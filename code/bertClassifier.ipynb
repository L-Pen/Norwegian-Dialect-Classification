{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juste\\OneDrive\\Desktop\\University\\COMP 550\\Final Project\\comp550-norwegian-dialects\\treebanks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\juste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\juste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from conllu import parse\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "notebook_path = Path().resolve()\n",
    "code_folder = notebook_path.parent\n",
    "treebanks_folder = code_folder / \"treebanks\"\n",
    "print(treebanks_folder)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "norwegian_stop_words = set(stopwords.words(\"norwegian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conll_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        conll_data = file.read()\n",
    "        return parse(conll_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juste\\OneDrive\\Desktop\\University\\COMP 550\\Final Project\\comp550-norwegian-dialects\\treebanks\n",
      "TokenList<kor, gammal, var, du, aal_uio_0201, da, du, begynte, å, køyre, tømmer, ?, metadata={text: \"kor gammal var du aal_uio_0201 da du begynte å køyre tømmer ?\", segstart: \"42.422\", segstop: \"45.249\", file: \"aal_uio_02\", speaker: \"khs\", id: \"2\"}>\n",
      "# text = kor gammal var du aal_uio_0201 da du begynte å køyre tømmer ?\n",
      "# segstart = 42.422\n",
      "# segstop = 45.249\n",
      "# file = aal_uio_02\n",
      "# speaker = khs\n",
      "# id = 2\n",
      "1\tkor\tkor\tadv\tadv\t_\t2\tADV\t_\t_\n",
      "2\tgammal\tgammal\tadj\tadj\teint|m/f|ub|pos\t3\tSPRED\t_\t_\n",
      "3\tvar\tvere\tverb\tverb\tpret\t0\tFINV\t_\t_\n",
      "4\tdu\tdu\tpron\tpron\tpers|eint|2|nom|hum\t3\tSUBJ\t_\t_\n",
      "5\taal_uio_0201\taal_uio_0201\tsubst\tsubst\tprop\t4\tAPP\t_\t_\n",
      "6\tda\tda\tsbu\tsbu\t_\t8\tSBU\t_\t_\n",
      "7\tdu\tdu\tpron\tpron\tpers|eint|2|nom|hum\t8\tSUBJ\t_\t_\n",
      "8\tbegynte\tbegynne\tverb\tverb\tpret\t3\tADV\t_\t_\n",
      "9\tå\tå\tinf-merke\tinf-merke\t_\t8\tDOBJ\t_\t_\n",
      "10\tkøyre\tkøyre\tverb\tverb\tinf\t9\tINFV\t_\t_\n",
      "11\ttømmer\ttømmer\tsubst\tsubst\tappell|fl|nøyt|ub\t10\tDOBJ\t_\t_\n",
      "12\t?\t$?\tclb\tclb\t<spm>\t3\tIP\t_\t_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In this function we will have a nested list all_parsed_data (nested list of sentences where the outter list is a specific dialect while the inner list is the sentences per dialect)\n",
    "#The second list all_parsed_data_filtered is the filtered version of the same thing\n",
    "#Each element in the inner list is a token and there is meta data at the end\n",
    "#Each token has associated info lemma,upos,etc\n",
    "all_parsed_data = [] \n",
    "print(treebanks_folder)\n",
    "\n",
    "for file_path in treebanks_folder.glob(\"*.conll\"):\n",
    "    parsed_data = parse_conll_file(file_path)\n",
    "    all_parsed_data.append(parsed_data)\n",
    "\n",
    "\n",
    "if all_parsed_data:\n",
    "    first_file_parsed_data = all_parsed_data[0][1]\n",
    "    serialized = first_file_parsed_data.serialize()\n",
    "    print(first_file_parsed_data)\n",
    "    print(serialized)\n",
    "    token = first_file_parsed_data[0]\n",
    "    # print(token['id'])\n",
    "    # print(token['form'])\n",
    "    # print(token['lemma'])\n",
    "    # print(token['upos'])\n",
    "else:\n",
    "    all_parsed_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_dataset(num_classes):\n",
    "    import pandas as pd\n",
    "    better_data ={}\n",
    "\n",
    "    for dialect in all_parsed_data:\n",
    "\n",
    "        dialect_name = dialect[0].metadata.get('file', '')\n",
    "        better_data[dialect_name] = dialect\n",
    "    \n",
    "    ls_aal = better_data['aal_uio_02'] + better_data['aal_uio_06']\n",
    "    ls_austevoll = better_data['austevoll_uib_01'] + better_data['austevoll_uib_04']\n",
    "\n",
    "    better_data['aal_uio_02'] = ls_aal\n",
    "    better_data['austevoll_uib_01'] = ls_austevoll\n",
    "\n",
    "    del better_data['aal_uio_06']\n",
    "    del better_data['austevoll_uib_04']\n",
    "\n",
    "    temp_data = {}\n",
    "    for dialect, token_list in better_data.items():\n",
    "        temp_ls = []\n",
    "        for token in token_list:\n",
    "            sentence = token.metadata.get('text', '')\n",
    "            if len(token) > 3:\n",
    "                temp_ls.append(sentence)\n",
    "        temp_data[dialect] = temp_ls\n",
    "\n",
    "\n",
    "    len_each_dialect = {}\n",
    "    for dialect, token_list in temp_data.items():\n",
    "        len_each_dialect[dialect] = len(token_list)\n",
    "    \n",
    "    sorted_len_each_dialect = sorted(len_each_dialect.items(), key=lambda x: x[1], reverse=True)[:num_classes]\n",
    "\n",
    "    filtered_dict = {key[0]: temp_data[key[0]] for key in sorted_len_each_dialect}\n",
    "    \n",
    "    #Aaustevoll, Bardu, Brandbu, Eidsberg, Fana, Lista, Flakstad, Førde, Giske, Gol, Hemsedal, Herad, Hjartdal, Høyanger, Nordli, Vardø and Ål\n",
    "    map_names = {\n",
    "        'bardu_uit_01': 'Bardu',\n",
    "        'brandbu_uio_01_ny': 'Brandbu',\n",
    "        'eidsberg_uio_03': 'Eidsberg',\n",
    "        'fana_uib_03': 'Fana',\n",
    "        'farsund_uib_02': 'Farsund',\n",
    "        'flakstad_uib_04': 'Flakstad', \n",
    "        'foerde_uib_05': 'Foerde',\n",
    "        'giske_uib_02': 'Giske',\n",
    "        'gol_uio_01': 'Gol',\n",
    "        'hemsedal_uio_01': 'Hemsedal',\n",
    "        'herad_uio_01': 'Herad',\n",
    "        'hjartdal_uio_01': 'Hjartdal',\n",
    "        'hoeyanger_uib_02': 'Hoeyanger',\n",
    "        'lierne_uio_01': 'Lierne', \n",
    "        'vardoe_uio_01': 'Vardoe',\n",
    "        'aal_uio_02': 'Al', \n",
    "        'austevoll_uib_01': 'Austevoll',\n",
    "    }\n",
    "    dialect_to_index = {dialect: idx for idx, dialect in enumerate(filtered_dict.keys())}\n",
    "    list_of_names_add_to_prompt = \", \".join([map_names[key] for key in filtered_dict.keys()])\n",
    "    # print(list_of_names_add_to_prompt)\n",
    "    prompt = f\"You are tasked with being a Norwegian Dialect classifier. The goal is to train a model that can accurately distinguish between different Norwegian dialects. The primary dialects of interest are {list_of_names_add_to_prompt} and you should be able to distinguish between these dialects.\\n Here is the sentence: \"\n",
    "    \n",
    "    test_size = 0.2\n",
    "\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "\n",
    "    for dialect, sentence_list in filtered_dict.items():\n",
    "        label = dialect_to_index[dialect]\n",
    "        train_size = int(len(sentence_list) * (1 - test_size))\n",
    "        train_dataset.extend([{\"label\": label, \"text\": sentence} for sentence in sentence_list[:train_size]])\n",
    "        test_dataset.extend([{\"label\": label, \"text\": sentence} for sentence in sentence_list[train_size:]])\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert_dataset3, test_bert_dataset3 = create_bert_dataset(3)\n",
    "train_bert_dataset6, test_bert_dataset6 = create_bert_dataset(6)\n",
    "train_bert_dataset12, test_bert_dataset12 = create_bert_dataset(12)\n",
    "train_bert_dataset17, test_bert_dataset17 = create_bert_dataset(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\juste\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "def tokenzie_data(train_bert_dataset,test_bert_dataset):\n",
    "#https://huggingface.co/docs/transformers/training\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"NbAiLab/notram-bert-norwegian-cased-080321\")\n",
    "\n",
    "    data_training = {\n",
    "        \"label\": [one[\"label\"] for one in train_bert_dataset],\n",
    "        \"text\": [one[\"text\"] for one in train_bert_dataset],\n",
    "    }\n",
    "    data_testing = {\n",
    "        \"label\": [one[\"label\"] for one in test_bert_dataset],\n",
    "        \"text\": [one[\"text\"] for one in test_bert_dataset],\n",
    "    }\n",
    "\n",
    "    num_classes = len(set(data_training[\"label\"]))\n",
    "    data_training[\"label\"] = [torch.nn.functional.one_hot(torch.tensor(label), num_classes=num_classes) for label in data_training[\"label\"]]\n",
    "    data_testing[\"label\"] = [torch.nn.functional.one_hot(torch.tensor(label), num_classes=num_classes) for label in data_testing[\"label\"]]\n",
    "\n",
    "    dataset_training = Dataset.from_dict(data_training)\n",
    "    dataset_testing = Dataset.from_dict(data_testing)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=50)\n",
    "\n",
    "    tokenized_dataset_training = dataset_training.map(tokenize_function)\n",
    "    tokenized_dataset_testing = dataset_testing.map(tokenize_function)\n",
    "    return tokenized_dataset_training, tokenized_dataset_testing, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bert_model(tokenized_dataset_training,tokenized_dataset_testing,num_classes):\n",
    "#https://huggingface.co/docs/transformers/training\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"NbAiLab/notram-bert-norwegian-cased-080321\", num_labels=num_classes)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"test_trainer\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "    )\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        references = np.argmax(labels, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset_training,\n",
    "        eval_dataset=tokenized_dataset_testing,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1665/1665 [00:00<00:00, 4070.90 examples/s]\n",
      "Map: 100%|██████████| 418/418 [00:00<00:00, 3906.56 examples/s]\n",
      "Map: 100%|██████████| 2595/2595 [00:00<00:00, 3782.50 examples/s]\n",
      "Map: 100%|██████████| 653/653 [00:00<00:00, 3934.88 examples/s]\n",
      "Map: 100%|██████████| 3999/3999 [00:01<00:00, 3779.76 examples/s]\n",
      "Map: 100%|██████████| 1006/1006 [00:00<00:00, 3698.53 examples/s]\n",
      "Map: 100%|██████████| 4748/4748 [00:01<00:00, 3465.20 examples/s]\n",
      "Map: 100%|██████████| 1196/1196 [00:00<00:00, 3908.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset_training3, tokenized_dataset_testing3, num_classes3 = tokenzie_data(train_bert_dataset3, test_bert_dataset3)\n",
    "tokenized_dataset_training6, tokenized_dataset_testing6, num_classes6 = tokenzie_data(train_bert_dataset6, test_bert_dataset6)\n",
    "tokenized_dataset_training12, tokenized_dataset_testing12, num_classes12 = tokenzie_data(train_bert_dataset12, test_bert_dataset12)\n",
    "tokenized_dataset_training17, tokenized_dataset_testing17, num_classes17 = tokenzie_data(train_bert_dataset17, test_bert_dataset17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = Path().resolve()\n",
    "code_folder = notebook_path.parent\n",
    "bertmodel_folder = code_folder / \"bertmodels3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/notram-bert-norwegian-cased-080321 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 33%|███▎      | 209/627 [09:15<14:33,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4014187455177307, 'eval_accuracy': 0.7870813397129187, 'eval_runtime': 20.3091, 'eval_samples_per_second': 20.582, 'eval_steps_per_second': 2.61, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 418/627 [19:17<08:52,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3799878656864166, 'eval_accuracy': 0.8038277511961722, 'eval_runtime': 23.8115, 'eval_samples_per_second': 17.555, 'eval_steps_per_second': 2.226, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 500/627 [23:26<06:26,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2507, 'learning_rate': 1.0127591706539077e-05, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 627/627 [30:34<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4020707309246063, 'eval_accuracy': 0.8253588516746412, 'eval_runtime': 25.2609, 'eval_samples_per_second': 16.547, 'eval_steps_per_second': 2.098, 'epoch': 3.0}\n",
      "{'train_runtime': 1834.7207, 'train_samples_per_second': 2.722, 'train_steps_per_second': 0.342, 'train_loss': 0.21306065128940904, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "bert_model3 = run_bert_model(tokenized_dataset_training3, tokenized_dataset_testing3, num_classes3)\n",
    "bert_model3.save_pretrained(bertmodel_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/notram-bert-norwegian-cased-080321 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 33%|███▎      | 325/975 [16:06<28:21,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32797539234161377, 'eval_accuracy': 0.5283307810107197, 'eval_runtime': 37.439, 'eval_samples_per_second': 17.442, 'eval_steps_per_second': 2.19, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 500/975 [25:38<25:36,  3.23s/it]  Checkpoint destination directory test_trainer\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2873, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 650/975 [34:09<14:23,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2832270562648773, 'eval_accuracy': 0.6339969372128637, 'eval_runtime': 37.9039, 'eval_samples_per_second': 17.228, 'eval_steps_per_second': 2.163, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 975/975 [49:46<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3057621717453003, 'eval_accuracy': 0.6600306278713629, 'eval_runtime': 34.176, 'eval_samples_per_second': 19.107, 'eval_steps_per_second': 2.399, 'epoch': 3.0}\n",
      "{'train_runtime': 2986.2839, 'train_samples_per_second': 2.607, 'train_steps_per_second': 0.326, 'train_loss': 0.2051008801582532, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "bert_model6 = run_bert_model(tokenized_dataset_training6, tokenized_dataset_testing6, num_classes6)\n",
    "bertmodel_folder = code_folder / \"bertmodels6\"\n",
    "bert_model6.save_pretrained(bertmodel_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/notram-bert-norwegian-cased-080321 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|███▎      | 500/1500 [22:25<46:08,  2.77s/it]  Checkpoint destination directory test_trainer\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2441, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 500/1500 [23:25<46:08,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22281526029109955, 'eval_accuracy': 0.3996023856858847, 'eval_runtime': 56.7044, 'eval_samples_per_second': 17.741, 'eval_steps_per_second': 2.222, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1000/1500 [46:57<22:41,  2.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1517, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 1000/1500 [47:56<22:41,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2079782783985138, 'eval_accuracy': 0.48906560636182905, 'eval_runtime': 56.6574, 'eval_samples_per_second': 17.756, 'eval_steps_per_second': 2.224, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [1:12:37<00:00,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0878, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1500/1500 [1:13:40<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20189645886421204, 'eval_accuracy': 0.5218687872763419, 'eval_runtime': 59.3681, 'eval_samples_per_second': 16.945, 'eval_steps_per_second': 2.122, 'epoch': 3.0}\n",
      "{'train_runtime': 4420.2015, 'train_samples_per_second': 2.714, 'train_steps_per_second': 0.339, 'train_loss': 0.16118491363525392, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "bert_model12 = run_bert_model(tokenized_dataset_training12, tokenized_dataset_testing12, num_classes12)\n",
    "bertmodel_folder = code_folder / \"bertmodels12\"\n",
    "bert_model12.save_pretrained(bertmodel_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/notram-bert-norwegian-cased-080321 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 28%|██▊       | 500/1782 [24:05<1:11:25,  3.34s/it]Checkpoint destination directory test_trainer\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2057, 'learning_rate': 3.5970819304152635e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 594/1782 [28:48<48:36,  2.45s/it]  \n",
      " 33%|███▎      | 594/1782 [29:56<48:36,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18718963861465454, 'eval_accuracy': 0.31187290969899667, 'eval_runtime': 68.2142, 'eval_samples_per_second': 17.533, 'eval_steps_per_second': 2.199, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1000/1782 [48:54<39:11,  3.01s/it] Checkpoint destination directory test_trainer\\checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1453, 'learning_rate': 2.1941638608305277e-05, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1188/1782 [57:40<24:42,  2.50s/it]\n",
      " 67%|██████▋   | 1188/1782 [58:43<24:42,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16739873588085175, 'eval_accuracy': 0.40217391304347827, 'eval_runtime': 63.233, 'eval_samples_per_second': 18.914, 'eval_steps_per_second': 2.372, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1500/1782 [1:10:53<12:23,  2.63s/it]Checkpoint destination directory test_trainer\\checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1064, 'learning_rate': 7.912457912457913e-06, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1782/1782 [1:22:37<00:00,  2.05s/it]\n",
      "100%|██████████| 1782/1782 [1:23:31<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17034120857715607, 'eval_accuracy': 0.41387959866220736, 'eval_runtime': 54.4092, 'eval_samples_per_second': 21.982, 'eval_steps_per_second': 2.757, 'epoch': 3.0}\n",
      "{'train_runtime': 5011.6074, 'train_samples_per_second': 2.842, 'train_steps_per_second': 0.356, 'train_loss': 0.14168905649923716, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "bert_model17 = run_bert_model(tokenized_dataset_training17, tokenized_dataset_testing17, num_classes17)\n",
    "bertmodel_folder = code_folder / \"bertmodels17\"\n",
    "bert_model17.save_pretrained(bertmodel_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
